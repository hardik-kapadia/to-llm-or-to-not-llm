{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer,AutoModel, BertTokenizer, BertModel\n",
    "from datasets import load_dataset, Dataset\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = load_dataset('imdb',split='train')\n",
    "imdb = imdb.shard(8, index=1)\n",
    "imdb.set_format(\"torch\",columns=['text','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(example):\n",
    "    wrds = example['text'].split(' ')\n",
    "    flts = [w for w in wrds if w.lower() not in stop_words]\n",
    "    str = \"\"\n",
    "    \n",
    "    for f in flts:\n",
    "        str+= f+\" \"\n",
    "    \n",
    "    new_one = {'text':str[:-1],'label':example['label']}\n",
    "    return new_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(imdb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb.map(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdbb = imdb.train_test_split(test_size=0.2,stratify_by_column='label')\n",
    "imdb_train = imdbb['train']\n",
    "imdb_test = imdbb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 12\n",
    "TEST_BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = imdb_test.shape[0] / TEST_BATCH_SIZE\n",
    "\n",
    "tokenized_test_batches = []\n",
    "tokenized_test_batches_y = []\n",
    "\n",
    "for i in range(int(n_test)):\n",
    "    btch = imdb_test['text'][TEST_BATCH_SIZE*i : min(TEST_BATCH_SIZE*(i+1), imdb_test.shape[0])]\n",
    "    tp = tokenizer.batch_encode_plus(btch,max_length=512, padding='max_length', truncation=True,return_tensors='pt')\n",
    "    tokenized_test_batches.append(tp)\n",
    "    tokenized_test_batches_y.append(imdb_test['label'][TEST_BATCH_SIZE*i : min(TEST_BATCH_SIZE*(i+1), imdb_test.shape[0])])\n",
    "    \n",
    "    if i % (n_test//10) == 0:\n",
    "        print(f' finished {int((i / (n_test)) * 100)}%')\n",
    "\n",
    "print(len(tokenized_test_batches))\n",
    "print(len(tokenized_test_batches_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBert(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(FBert,self).__init__()\n",
    "        self.l1 = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.2)\n",
    "        self.l3 = torch.nn.Linear(768,64)\n",
    "        self.l4 = torch.nn.Linear(64,1)\n",
    "        self.l5 = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        ids = input['input_ids']\n",
    "        tto = input['token_type_ids']\n",
    "        attn = input['attention_mask']\n",
    "        \n",
    "        _, t = self.l1(ids, attention_mask = attn, token_type_ids = tto, return_dict=False)\n",
    "        # print(f'a1[0] -> {t.size()}')\n",
    "        a2 = self.l2(t)\n",
    "        # print(f'a2 -> {a2.size()}')\n",
    "        a3 = self.l3(a2)\n",
    "        # print(f' a3 -> {a3.size()}')\n",
    "        a4 = self.l4(a3)\n",
    "        # print(f' a4 -> {a4.size()}')\n",
    "        a5 = self.l5(a4)\n",
    "        # print(f' a5 -> {a5.size()}')\n",
    "        a6 = a5.squeeze()\n",
    "        # print(f' a6 -> {a6.size()}')\n",
    "        return a6\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, epoch=0):\n",
    "    \n",
    "    loss_graph = []\n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        for b, b_y in zip(tokenized_test_batches, tokenized_test_batches_y):\n",
    "            b.to(device)\n",
    "            test_pred = model(b)\n",
    "            actual_test = torch.as_tensor(b_y,device=device,dtype=torch.float)\n",
    "            ts_ls = loss_fn(test_pred, actual_test)\n",
    "            losses.append(ts_ls)\n",
    "            total_loss += ts_ls\n",
    "            count +=1\n",
    "            print(f'Batch {count}, Test loss: {total_loss/count}')\n",
    "            loss_graph.append(total_loss/count)\n",
    "            \n",
    "        total_loss /= len(tokenized_test_batches)\n",
    "        test_loss[epoch] = total_loss\n",
    "        print(f'Epoch {epoch}, Test loss: {total_loss}')\n",
    "        return total_loss, loss_graph, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_preds(model, epoch=0):\n",
    "    \n",
    "    predicted = []\n",
    "    actual = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        for b, b_y in zip(tokenized_test_batches, tokenized_test_batches_y):\n",
    "            b.to(device)\n",
    "            test_pred = model(b)\n",
    "            predicted.extend(test_pred.cpu().tolist())\n",
    "            actual.extend(b_y.flatten().tolist())\n",
    "\n",
    "\n",
    "        return predicted,actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm = torch.load(\"Models/fbert.pth\")\n",
    "nm = nm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,a = evaluate_preds(nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(p),len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(p))\n",
    "pclass = [1 if x > 0.5 else 0 for x in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(pclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(p,a):\n",
    "    ps = precision_score(a,p)\n",
    "    rs = recall_score(a,p)\n",
    "    f1 = f1_score(a,p)\n",
    "    ac = accuracy_score(a,p)\n",
    "    \n",
    "    return ps,rs,f1,ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculate_metrics(pclass,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model = nm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlprep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
