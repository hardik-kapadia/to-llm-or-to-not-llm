{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer,AutoModel, BertTokenizer, BertModel\n",
    "from datasets import load_dataset, Dataset\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kapsu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = load_dataset('imdb',split='train')\n",
    "imdb = imdb.shard(8, index=1)\n",
    "imdb.set_format(\"torch\",columns=['text','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(example):\n",
    "    wrds = example['text'].split(' ')\n",
    "    flts = [w for w in wrds if w.lower() not in stop_words]\n",
    "    str = \"\"\n",
    "    \n",
    "    for f in flts:\n",
    "        str+= f+\" \"\n",
    "    \n",
    "    new_one = {'text':str[:-1],'label':example['label']}\n",
    "    return new_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(imdb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 3125\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.map(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdbb = imdb.train_test_split(test_size=0.2,stratify_by_column='label')\n",
    "imdb_train = imdbb['train']\n",
    "imdb_test = imdbb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 12\n",
    "TEST_BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = imdb_train.shape[0] / TRAIN_BATCH_SIZE\n",
    "tokenized_train_batches = []\n",
    "tokenized_train_batches_y = []\n",
    "\n",
    "for i in range(int(n_train)):\n",
    "    btch = imdb_train['text'][TRAIN_BATCH_SIZE*i : min(TRAIN_BATCH_SIZE*(i+1), imdb_train.shape[0])]\n",
    "    tp = tokenizer.batch_encode_plus(btch,max_length=512, padding='max_length', truncation=True,return_tensors='pt')\n",
    "    tokenized_train_batches.append(tp)\n",
    "    tokenized_train_batches_y.append(imdb_train['label'][TRAIN_BATCH_SIZE*i : min(TRAIN_BATCH_SIZE*(i+1), imdb_train.shape[0])])\n",
    "    \n",
    "    if i % (n_train//10) == 0:\n",
    "        print(f' finished {int((i / (n_train)) * 100)}%')\n",
    "\n",
    "print(len(tokenized_train_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " finished 0%\n",
      " finished 9%\n",
      " finished 19%\n",
      " finished 28%\n",
      " finished 38%\n",
      " finished 48%\n",
      " finished 57%\n",
      " finished 67%\n",
      " finished 76%\n",
      " finished 86%\n",
      " finished 96%\n",
      "156\n",
      "156\n"
     ]
    }
   ],
   "source": [
    "n_test = imdb_test.shape[0] / TEST_BATCH_SIZE\n",
    "\n",
    "tokenized_test_batches = []\n",
    "tokenized_test_batches_y = []\n",
    "\n",
    "for i in range(int(n_test)):\n",
    "    btch = imdb_test['text'][TEST_BATCH_SIZE*i : min(TEST_BATCH_SIZE*(i+1), imdb_test.shape[0])]\n",
    "    tp = tokenizer.batch_encode_plus(btch,max_length=512, padding='max_length', truncation=True,return_tensors='pt')\n",
    "    tokenized_test_batches.append(tp)\n",
    "    tokenized_test_batches_y.append(imdb_test['label'][TEST_BATCH_SIZE*i : min(TEST_BATCH_SIZE*(i+1), imdb_test.shape[0])])\n",
    "    \n",
    "    if i % (n_test//10) == 0:\n",
    "        print(f' finished {int((i / (n_test)) * 100)}%')\n",
    "\n",
    "print(len(tokenized_test_batches))\n",
    "print(len(tokenized_test_batches_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBert(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(FBert,self).__init__()\n",
    "        self.l1 = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.2)\n",
    "        self.l3 = torch.nn.Linear(768,64)\n",
    "        self.l4 = torch.nn.Linear(64,1)\n",
    "        self.l5 = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        ids = input['input_ids']\n",
    "        tto = input['token_type_ids']\n",
    "        attn = input['attention_mask']\n",
    "        \n",
    "        _, t = self.l1(ids, attention_mask = attn, token_type_ids = tto, return_dict=False)\n",
    "        # print(f'a1[0] -> {t.size()}')\n",
    "        a2 = self.l2(t)\n",
    "        # print(f'a2 -> {a2.size()}')\n",
    "        a3 = self.l3(a2)\n",
    "        # print(f' a3 -> {a3.size()}')\n",
    "        a4 = self.l4(a3)\n",
    "        # print(f' a4 -> {a4.size()}')\n",
    "        a5 = self.l5(a4)\n",
    "        # print(f' a5 -> {a5.size()}')\n",
    "        a6 = a5.squeeze()\n",
    "        # print(f' a6 -> {a6.size()}')\n",
    "        return a6\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = FBert()\n",
    "my_model = my_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCELoss()\n",
    "optim = torch.optim.Adam(my_model.parameters(),lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch=0):\n",
    "    my_model.train()\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for batch, batch_y in zip(tokenized_train_batches, tokenized_train_batches_y):\n",
    "        batch.to(device)\n",
    "        preds = my_model(batch)\n",
    "        \n",
    "        actual = torch.as_tensor(batch_y,device=device,dtype=torch.float)\n",
    "        loss = loss_fn(preds,actual)\n",
    "\n",
    "        optim.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "        \n",
    "        if count % 20 == 0:\n",
    "            print(f'onto batch: {count}')\n",
    "            \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model = my_model, epoch=0):\n",
    "    \n",
    "    loss_graph = []\n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        my_model.eval()\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        for b, b_y in zip(tokenized_test_batches, tokenized_test_batches_y):\n",
    "            b.to(device)\n",
    "            test_pred = my_model(b)\n",
    "            actual_test = torch.as_tensor(b_y,device=device,dtype=torch.float)\n",
    "            ts_ls = loss_fn(test_pred, actual_test)\n",
    "            losses.append(ts_ls)\n",
    "            total_loss += ts_ls\n",
    "            count +=1\n",
    "            print(f'Batch {count}, Test loss: {total_loss/count}')\n",
    "            loss_graph.append(total_loss/count)\n",
    "            \n",
    "        total_loss /= len(tokenized_test_batches)\n",
    "        test_loss[epoch] = total_loss\n",
    "        print(f'Epoch {epoch}, Test loss: {total_loss}')\n",
    "        return total_loss, loss_graph, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_preds(model, epoch=0):\n",
    "    \n",
    "    predicted = []\n",
    "    actual = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        my_model.eval()\n",
    "        \n",
    "        for b, b_y in zip(tokenized_test_batches, tokenized_test_batches_y):\n",
    "            b.to(device)\n",
    "            test_pred = model(b)\n",
    "            predicted.extend(test_pred.cpu().tolist())\n",
    "            actual.extend(b_y.flatten().tolist())\n",
    "\n",
    "\n",
    "        return predicted,actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(my_model,'Models/fbert.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model = my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,a = evaluate_preds(my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624 624\n"
     ]
    }
   ],
   "source": [
    "print(len(p),len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(p))\n",
    "pclass = [1 if x > 0.5 else 0 for x in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(pclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(p,a):\n",
    "    ps = precision_score(a,p)\n",
    "    rs = recall_score(a,p)\n",
    "    f1 = f1_score(a,p)\n",
    "    ac = accuracy_score(a,p)\n",
    "    \n",
    "    return ps,rs,f1,ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9201277955271565, 0.9230769230769231, 0.9216, 0.9214743589743589)\n"
     ]
    }
   ],
   "source": [
    "print(calculate_metrics(pclass,a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlprep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
